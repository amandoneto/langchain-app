from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from openai import OpenAI as OpenAIClient
from utils.env_loader import EnvLoader

"""
This example is to demonstrate how to use LangChain's OpenAI integration for streaming and standard completions.
It showcases two primary functionalities:
1. Streaming Completions: Using the streaming interface of the LangChain OpenAI LLM to receive and print output in real-time.
2. Standard Completions: Making a standard call to the OpenAI LLM and retrieving the full response.
"""
class CompareLangChainOpenAI:
    

    def __init__(self):
        pass


    def run_openai_streaming(self, model_name: str, api_key: str, prompt: str):
        """
        To begin interacting with the OpenAI API, we need to format a list of messages. Each message is represented as a dictionary containing two primary keys: role and content. 
        The role indicates who is sending the message (it can be user, assistant, or system), and the content contains the message itself.

        Args:
                None
        Returns:
                None
        Side effects:
                - Prints the streamed output to stdout.
                - Blocks until the streaming generator completes.
        Notes:
                - The method expects the OpenAI API key and any other configuration to be provided
                    elsewhere (e.g., via environment variables and the class initializer).
                - The type and shape of each yielded item depends on the LLM implementation; this
                    implementation assumes each yielded chunk can be directly converted to a string.
                - For production use, consider parameterizing the prompt, handling partial tokens,
                    adding error handling, and making the LLM configuration (model, temperature, etc.)
                    explicit.
        Raises:
                RuntimeError: if the OpenAI client initialization fails due to missing configuration.
                Exception: may propagate exceptions raised by the underlying LLM streaming call.
        
    """
        try:
            client = OpenAIClient(api_key=api_key)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=False
            )
            print(response.choices[0].message.content)
            
            print()
        except Exception as e:
            print(f"OpenAI an error occurred during streaming: {e}")

    def run_langchain(self, model_name: str, api_key: str, prompt: str) -> str:
        """
        Now, if we opt to use LangChain, message generation becomes more intuitive. LangChain utilizes specific classes to represent messages, 
        which simplifies their creation and management. Take a look at how simple it is to generate a message using LangChain.
        
        Args:
                prompt (str): The input prompt to send to the LLM.
        Returns:
                str: The response generated by the LLM.
        Raises:
                RuntimeError: if the OpenAI client initialization fails due to missing configuration.
                Exception: may propagate exceptions raised by the underlying LLM call.
        Example:
                response = self.call_openai("What is the capital of France?")
                print(response)  # e.g., "The capital of France is Paris."
        """
        mensagens = [
            HumanMessage(content=prompt)
        ]
        try:
            llm = ChatOpenAI(model_name=model_name, api_key=api_key)
            response = llm.invoke(mensagens)
            print(response.content)
            
            print()
        except Exception as e:
            print(f"OpenAI an error occurred: {e}")
            return ""
        
    
        
if __name__ == "__main__":
    config = EnvLoader()
    model_name = config.get_required("OPENAI_MODEL_NAME")
    api_key = config.get_required("OPENAI_API_KEY")
    prompt = "Can you explain the latest version of Java?"
    
    comparer = CompareLangChainOpenAI()
    print("**************** OPENAI ****************")
    comparer.run_openai_streaming(model_name, api_key, prompt)
    print("\n---\n")
    print("**************** LANGCHAIN ****************")
    comparer.run_langchain(model_name, api_key, prompt)
    print("\n---\n")
    
